{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Azure ML is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"Ready to use Azure ML\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "WARNING - You have logged in. Now let us find all the subscriptions to which you have access...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n",
      "LEAF_ESP_workspace loaded\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.get(name='LEAF_ESP_workspace',\n",
    "                   subscription_id='8e8a0cc2-ae82-484e-84df-2abb7bf63bc1',\n",
    "                   resource_group='LEAF_ESP_resource_group')\n",
    "print(ws.name, \"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the resources within your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I already had imported the sample data into my workspace. But it can easily be imported through the SDK as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Targets:\n",
      "\t LEAF-ESP-compute : ComputeInstance\n",
      "\t LEAF-cluster : AmlCompute\n",
      "Datastores:\n",
      "\t leaf_data : AzureBlob\n",
      "\t workspaceblobstore : AzureBlob\n",
      "\t workspacefilestore : AzureFile\n",
      "Datasets:\n",
      "\t leaf_test\n",
      "\t leaf_train\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import ComputeTarget, Datastore, Dataset\n",
    "\n",
    "print(\"Compute Targets:\")\n",
    "for compute_name in ws.compute_targets:\n",
    "    compute = ws.compute_targets[compute_name]\n",
    "    print(\"\\t\", compute.name, ':', compute.type)\n",
    "    \n",
    "print(\"Datastores:\")\n",
    "for datastore_name in ws.datastores:\n",
    "    datastore = Datastore.get(ws, datastore_name)\n",
    "    print(\"\\t\", datastore.name, ':', datastore.datastore_type)\n",
    "    \n",
    "print(\"Datasets:\")\n",
    "for dataset_name in list(ws.datasets.keys()):\n",
    "    dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "    print(\"\\t\", dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nickm\\\\Desktop\\\\Cognizant\\\\Python Repository\\\\Cloud Technologies\\\\Azure'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf-training\\\\leaf_train.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a folder for the experiment files\n",
    "training_folder = 'leaf-training'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "# Copy the data file into the experiment folder\n",
    "shutil.copy('C:\\\\Users\\\\nickm\\\\Desktop\\\\Cognizant\\\\Python Repository\\\\Evolutionary AI\\\\esp-xde\\\\notebooks\\\\1.7\\\\data\\\\train.csv', os.path.join(training_folder, \"leaf_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leaf-training\\\\leaf_test.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy('C:\\\\Users\\\\nickm\\\\Desktop\\\\Cognizant\\\\Python Repository\\\\Evolutionary AI\\\\esp-xde\\\\notebooks\\\\1.7\\\\data\\\\test.csv', os.path.join(training_folder, \"leaf_test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the predictor model training script which will later be referenced as a pipeline step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing leaf-training/leaf_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/leaf_training.py\n",
    "# Import libraries\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let Pandas display all the columns. We have 49 columns in our dataset.\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_folder', type=str, dest='output_folder', default=\"predictor_model\", help='output folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.output_folder\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes data (passed as an input dataset)\n",
    "print(\"Loading Data...\")\n",
    "cwd = os.getcwd()\n",
    "train_df = pd.read_csv(cwd+'\\\\leaf-training\\\\leaf_train.csv').drop(columns='Unnamed: 0')\n",
    "test_df = pd.read_csv(cwd+'\\\\leaf-training\\\\leaf_test.csv').drop(columns='Unnamed: 0')\n",
    "train_df.head()\n",
    "\n",
    "# print(f\"Train: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "# print(f\"Test: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n",
    "\n",
    "OUTCOME_CATEGORIES = ['Very bad', 'Bad', 'Neutral', 'Good', 'Very good']\n",
    "def set_outcome_categories(df):\n",
    "    df[\"Cost\"] = pd.Categorical(df[\"Cost\"], categories=OUTCOME_CATEGORIES,ordered=True)\n",
    "    df[\"Schedule\"] = pd.Categorical(df[\"Schedule\"], categories=OUTCOME_CATEGORIES,ordered=True)\n",
    "    df[\"Quality\"] = pd.Categorical(df[\"Quality\"], categories=OUTCOME_CATEGORIES,ordered=True)\n",
    "\n",
    "set_outcome_categories(train_df)\n",
    "set_outcome_categories(test_df)\n",
    "\n",
    "# Train\n",
    "train_X_df = train_df.drop(['Cost','Schedule','Quality'], axis=1)\n",
    "train_Y_df = train_df[['Cost','Schedule','Quality']]\n",
    "# 3 labels: cost, schedule and quality\n",
    "train_cost_df = train_df[['Cost']]\n",
    "train_schedule_df = train_df[['Schedule']]\n",
    "train_quality_df = train_df[['Quality']]\n",
    "\n",
    "# Test\n",
    "test_X_df = test_df.drop(['Cost','Schedule','Quality'], axis=1)\n",
    "test_Y_df = test_df[['Cost','Schedule','Quality']]\n",
    "# 3 labels: cost, schedule and quality\n",
    "test_cost_df = test_df[['Cost']]\n",
    "test_schedule_df = test_df[['Schedule']]\n",
    "test_quality_df = test_df[['Quality']]\n",
    "\n",
    "def encode_dataset(reference_df, to_encode_df):\n",
    "    \"\"\"\n",
    "    Encodes the passed dataset and makes it contains the same columns, in the same order, as the reference one.\n",
    "    See https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data\n",
    "    \"\"\"\n",
    "    encoded_df = pd.get_dummies(to_encode_df)\n",
    "    # Get missing columns in the encoded dataset\n",
    "    missing_cols = set(reference_df.columns ) - set(encoded_df.columns )\n",
    "    # Add missing columns in encoded set with default value equal to 0\n",
    "    for c in missing_cols:\n",
    "        encoded_df[c] = 0\n",
    "    # Ensure columns in the encoded set are in the same order as in the reference set\n",
    "    encoded_df = encoded_df[reference_df.columns]\n",
    "    return encoded_df\n",
    "\n",
    "# These dataframes MUST contain all the possible values\n",
    "# It becomes the \"reference\" in terms of encoded columns\n",
    "encoded_train_X_df = pd.get_dummies(train_X_df)\n",
    "\n",
    "# Test set. Make sure we have the SAME columns as in the train set\n",
    "encoded_test_X_df = encode_dataset(encoded_train_X_df, test_X_df)\n",
    "\n",
    "# train_X_df.head()\n",
    "\n",
    "#encoded_test_X_df.head()\n",
    "\n",
    "# train_Y_df.head()\n",
    "\n",
    "encoded_context_actions_column_names = list(encoded_train_X_df.columns)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(cwd+\"\\\\leaf-training\\\\encoded_context_actions_column_names.csv\", \"w\") as csv_file:\n",
    "     wr = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerow(encoded_context_actions_column_names)\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# n_jobs=-1 means using all processors\n",
    "rfc = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, n_jobs=-1,  random_state=45, bootstrap=False))\n",
    "rfc.fit(encoded_train_X_df, train_Y_df)\n",
    "\n",
    "# Convert y to a 1d array\n",
    "train_cost_df = train_cost_df.values[:,0]\n",
    "rfc_cost = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rfc_cost.fit(encoded_train_X_df, train_cost_df)\n",
    "\n",
    "# Convert y to a 1d array\n",
    "train_schedule_df = train_schedule_df.values[:,0]\n",
    "rfc_schedule = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rfc_schedule.fit(encoded_train_X_df, train_schedule_df)\n",
    "\n",
    "# Convert y to a 1d array\n",
    "train_quality_df = train_quality_df.values[:,0]\n",
    "rfc_quality = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rfc_quality.fit(encoded_train_X_df, train_quality_df)\n",
    "\n",
    "preds = rfc_cost.predict(encoded_test_X_df)\n",
    "preds2 = rfc_quality.predict(encoded_test_X_df)\n",
    "preds3 = rfc_schedule.predict(encoded_test_X_df)\n",
    "# preds\n",
    "\n",
    "print('cost model accuracy: ',accuracy_score(test_cost_df, preds))\n",
    "print('quality model accuracy: ',accuracy_score(test_quality_df, preds2))\n",
    "print('schedule model accuracy: ',accuracy_score(test_schedule_df, preds3))\n",
    "\n",
    "# Save the trained models\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path1 = output_folder + \"/all_preds_model.pkl\"\n",
    "joblib.dump(value=rfc, filename=output_path1)\n",
    "output_path2 = output_folder + \"/cost_model.pkl\"\n",
    "joblib.dump(value=rfc_cost, filename=output_path2)\n",
    "output_path3 = output_folder + \"/quality_model.pkl\"\n",
    "joblib.dump(value=rfc_quality, filename=output_path3)\n",
    "output_path4 = output_folder + \"/schedule_model.pkl\"\n",
    "joblib.dump(value=rfc_schedule, filename=output_path4)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script for the second step of the pipeline will load the model from where it was saved, and then register it in the workspace. It includes a single model_folder parameter that contains the path where the model was saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing leaf-training/register_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/register_predictor.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Workspace, Model, Run\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_folder', type=str, dest='model_folder', default=\"predictor_model\", help='model location')\n",
    "args = parser.parse_args()\n",
    "model_folder = args.model_folder\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the model\n",
    "print(\"Loading all outcome model from \" + model_folder)\n",
    "model_file1 = model_folder + \"/all_preds_model.pkl\"\n",
    "all_outcome_model = joblib.load(model_file1)\n",
    "\n",
    "print(\"Loading cost model from \" + model_folder)\n",
    "model_file2 = model_folder + \"/cost_model.pkl\"\n",
    "cost_model = joblib.load(model_file2)\n",
    "\n",
    "print(\"Loading quality model from \" + model_folder)\n",
    "model_file3 = model_folder + \"/quality_model.pkl\"\n",
    "quality_model = joblib.load(model_file3)\n",
    "\n",
    "print(\"Loading schedule model from \" + model_folder)\n",
    "model_file4 = model_folder + \"/schedule_model.pkl\"\n",
    "schedule_model = joblib.load(model_file4)\n",
    "\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file1,\n",
    "               model_name = 'all_outcome_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file2,\n",
    "               model_name = 'cost_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file3,\n",
    "               model_name = 'quality_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file4,\n",
    "               model_name = 'schedule_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a compute environment for the pipeline steps\n",
    "In this exercise, you'll use the same compute for both steps, but it's important to realize that each step is run independently; so you could specify different compute contexts for each step if appropriate.\n",
    "\n",
    "First, get the compute target. We already created a compute target which can be seen in Studio. For our purposes, let's just use that one. But we can provision code to configure a new one in case we need to delete the old one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"LEAF-cluster\"\n",
    "\n",
    "# Verify that cluster exists\n",
    "try:\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If not, create it\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS1_V2', \n",
    "                                                           max_nodes=2)\n",
    "    pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "pipeline_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
